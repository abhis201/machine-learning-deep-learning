{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d531447",
   "metadata": {},
   "source": [
    "## CIFAR 10\n",
    "\n",
    "* classification (10 classes) \n",
    "\n",
    "* CIFAR 10 consists of: 60,000 tiny 32 x 32 color RGB images\n",
    "\n",
    "* labeled with integer 1 to 10 classes \n",
    "\n",
    "* airplane (0), car (1), etc. \n",
    "\n",
    "* 50000 -> training, 10000 -> testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ec8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "## import imageio\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39a0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/cifar10data/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395a6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_test = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c597c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b830b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "print(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbec4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = cifar10[79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315b8b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c5ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD06+8QX8BhFqqzSS4HlqQVHXvwf89q4XxX4s8VaJA2p+XI8Sth0LYVB64AyR+NaWkeN/tdmbXV7eH7ZbOYZ1EQKFx3B7Z64zVy9i0PX/s/2xruNLdi2yK4by2BGCGU8EYqbSvoLRlPwt8Vor+KIXk2ZGxv3gKAfbFega5cpL4V1KWJypNlK69mHyHHHUV4toUPhjTtYmtfCr2eoXsLMQJ1YTDHXy2b5Tj/AGeavaj4t1WW11G0nicH7O4dHGGTKGr3Jimt2eoaZ4I0OwtZontlu3nlaWSa4VS5J9wBgAAAAVHdeBNHmRxEZ7fcCDskyOf97NZ95r8A1KU2finSI7d/mxJfRnBxjGDnH/6+tRDWoDL+98XaRLEGD4+3ohY+ny4wP8KRZw198PNQ02SxsNIsLia4tL8ul5tAVomySST8vQkEZzmrVx8N9XOmapc6ldLBDFbSMAjbmfCk9AcD9a61/EDqUZPGGiNlsMv2uHAGOucc80268QWf/CP6qL7xPpNxI9hKipFeR4IB/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f0e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AugMix', 'AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose', 'ConvertImageDtype', 'ElasticTransform', 'FiveCrop', 'GaussianBlur', 'Grayscale', 'InterpolationMode', 'Lambda', 'LinearTransformation', 'Normalize', 'PILToTensor', 'Pad', 'RandAugment', 'RandomAdjustSharpness', 'RandomAffine', 'RandomApply', 'RandomAutocontrast', 'RandomChoice', 'RandomCrop', 'RandomEqualize', 'RandomErasing', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomInvert', 'RandomOrder', 'RandomPerspective', 'RandomPosterize', 'RandomResizedCrop', 'RandomRotation', 'RandomSolarize', 'RandomVerticalFlip', 'Resize', 'TenCrop', 'ToPILImage', 'ToTensor', 'TrivialAugmentWide', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_functional_pil', '_functional_tensor', '_presets', 'autoaugment', 'functional', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "print(   dir(transforms)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30cd7557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x12F795F10>\n"
     ]
    }
   ],
   "source": [
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "297cacc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "\n",
    "print(img_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ee615",
   "metadata": {},
   "source": [
    "The transforms can be passed directly to tnhe entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0a2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc67577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t , label = tensor_cifar10[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c1647d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[[0.2431, 0.1961, 0.1804,  ..., 0.6549, 0.7176, 0.5373],\n",
      "         [0.2471, 0.2157, 0.2039,  ..., 0.6392, 0.6706, 0.5686],\n",
      "         [0.2275, 0.2510, 0.2196,  ..., 0.6000, 0.5882, 0.4824],\n",
      "         ...,\n",
      "         [0.6745, 0.5608, 0.5098,  ..., 0.3686, 0.5529, 0.5451],\n",
      "         [0.7176, 0.5882, 0.3137,  ..., 0.3176, 0.5294, 0.5608],\n",
      "         [0.8196, 0.7137, 0.5451,  ..., 0.2314, 0.5098, 0.6627]],\n",
      "\n",
      "        [[0.2510, 0.1961, 0.1725,  ..., 0.6745, 0.7216, 0.5333],\n",
      "         [0.2549, 0.2078, 0.1961,  ..., 0.6627, 0.6824, 0.5725],\n",
      "         [0.2431, 0.2588, 0.2353,  ..., 0.6078, 0.6039, 0.5020],\n",
      "         ...,\n",
      "         [0.5294, 0.4314, 0.2196,  ..., 0.2941, 0.4235, 0.4118],\n",
      "         [0.5725, 0.4627, 0.2510,  ..., 0.2824, 0.4627, 0.4902],\n",
      "         [0.6824, 0.5922, 0.4275,  ..., 0.2118, 0.4667, 0.6118]],\n",
      "\n",
      "        [[0.1725, 0.1020, 0.0745,  ..., 0.2706, 0.2980, 0.2824],\n",
      "         [0.1451, 0.1020, 0.1059,  ..., 0.2392, 0.2941, 0.3020],\n",
      "         [0.1412, 0.1451, 0.1451,  ..., 0.2431, 0.2510, 0.2235],\n",
      "         ...,\n",
      "         [0.3882, 0.3294, 0.1647,  ..., 0.2196, 0.3373, 0.3176],\n",
      "         [0.4588, 0.3725, 0.1725,  ..., 0.2353, 0.3843, 0.4314],\n",
      "         [0.5647, 0.4824, 0.3255,  ..., 0.1843, 0.4353, 0.6275]]])\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "print(img_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e3281",
   "metadata": {},
   "source": [
    "## Normalize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f291ff70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imgs_list = [ img_t for img_t, label in tensor_cifar10 ]\n",
    "\n",
    "imgs = torch.stack( imgs_list, dim=3 )\n",
    "\n",
    "imgs_list[7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abec28ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n"
     ]
    }
   ],
   "source": [
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac3226fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51200000])\n"
     ]
    }
   ],
   "source": [
    "view_means = imgs.view(3, -1)\n",
    "print(view_means.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149cafef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n"
     ]
    }
   ],
   "source": [
    "view_means = view_means.mean(dim=1)\n",
    "print(view_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d08b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "view_std_dev = imgs.view(3, -1).std(dim=1)\n",
    "print(view_std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e50636-b3be-4c39-a505-ecdb51be15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10_train = datasets.CIFAR10(data_path, train=True, download= False,\n",
    "                                          transform = transforms.Compose(\n",
    "                                              [\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(view_means, view_std_dev)\n",
    "                                              ]\n",
    "                                          )\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "105da5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel = pixel - mean/std #Normalization\n",
    "\n",
    "transformed_cifar10_test = datasets.CIFAR10(data_path, train=False, download= False,\n",
    "                                          transform = transforms.Compose(\n",
    "                                              [\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(view_means, view_std_dev)\n",
    "                                              ]\n",
    "                                          )\n",
    "                                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f9721",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4098299b-8e86-458a-9ddb-2e1193876620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {0:0, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02ba4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_train = [(img, label_map[label]) for img, label in transformed_cifar10_train if label in [0,2]]\n",
    "cifar_train = [(img, label) for img, label in transformed_cifar10_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d1c68ff-7a6a-4c20-82ea-5732145de159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = [(img, label_map[label]) for img, label in transformed_cifar10_test if label in [0,2]]\n",
    "cifar_test = [(img, label) for img, label in transformed_cifar10_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b316a0a",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1aa17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63a8ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 4., 3.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 4.0, 3.0 ])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca67c34f-daab-444c-9740-2d493a5a4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.exp(x);\n",
    "# x = x/x.sum()\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8714580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0351, 0.7054, 0.2595])\n"
     ]
    }
   ],
   "source": [
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50111b78",
   "metadata": {},
   "source": [
    "\n",
    "## Architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2da75326",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = nn.Sequential(\n",
    "          nn.Linear(3072, 200),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(200, 10),\n",
    "          nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c07193b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_3DL = nn.Sequential(\n",
    "     nn.Linear(3072, 2048),\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(2048, 1024),\n",
    "     nn.GELU(),\n",
    "     nn.Linear(1024, 512),\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(512, 128),\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(128, 10),\n",
    "     nn.LogSoftmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd78dd",
   "metadata": {},
   "source": [
    "\n",
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6565a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar_train, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6f17af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_fn = model_mlp\n",
    "model_fn = model_3DL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa28ee",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ea2dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "learning_rate = 0.001  ## 1e-2    ## 0.001\n",
    "optimizer = optim.Adam(  model_fn.parameters(), lr=learning_rate )\n",
    "\n",
    "# optimizer = optim.SGD(  model_fn.parameters(), lr=learning_rate )\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84636e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:       ## imgs [batch_sizex3x32x32 = 3*1024]\n",
    "        ## print(imgs.shape)\n",
    "        ## resize for network\n",
    "        batch_size = imgs.shape[0]\n",
    "        imgs_resized = imgs.view(batch_size, -1)    ## imgs_resized [batch_size, 3072]\n",
    "        ## print(imgs_resized.shape)\n",
    "        outputs = model_fn(imgs_resized)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss)\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db55338",
   "metadata": {},
   "source": [
    "## Testing after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33fcd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_DataLoader = torch.utils.data.DataLoader(cifar_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0001984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4375)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_DataLoader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs    = model_fn(    imgs.view(batch_size, -1)     )\n",
    "        vals, indices = torch.max(outputs, dim=1)\n",
    "        preds = indices\n",
    "        metric = (preds == labels).sum()\n",
    "        total = batch_size\n",
    "\n",
    "print(metric/total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e52f3a",
   "metadata": {},
   "source": [
    "## All performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3afe1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getF1(algo_name, y_test, y_pred):\n",
    "    return f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "\n",
    "def print_stats_percentage_train_test(algorithm_name, y_test, y_pred):    \n",
    "     print(\"algorithm is: \", algorithm_name)\n",
    "     print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )\n",
    "     confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "     print(\"confusion matrix\")\n",
    "     print(confmat)\n",
    "     print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "     print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "     print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "535379c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max F1-Measure: 0.7422488306219923\n",
      "algorithm is:  4DL\n",
      "Accuracy: 0.75\n",
      "confusion matrix\n",
      "[[3 0 1 0 0 0 0 0 1 0]\n",
      " [0 4 0 0 0 0 0 0 0 1]\n",
      " [1 0 3 0 1 2 0 0 0 0]\n",
      " [0 0 0 3 1 1 1 0 0 0]\n",
      " [0 0 0 0 5 0 0 0 0 0]\n",
      " [0 0 1 0 0 8 0 0 0 0]\n",
      " [0 0 0 1 0 0 2 0 0 0]\n",
      " [0 0 1 0 0 1 0 6 0 1]\n",
      " [0 0 0 0 0 0 0 0 7 0]\n",
      " [0 0 0 0 0 0 0 1 0 7]]\n",
      "Precision: 0.756\n",
      "Recall: 0.750\n",
      "F1-measure: 0.742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    maxf1 = 0\n",
    "    max_y_test = []\n",
    "    max_y_pred = []\n",
    "    for imgs, labels in test_DataLoader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs    = model_fn(    imgs.view(batch_size, -1)     )\n",
    "        vals, preds = torch.max(  outputs, dim=1  )\n",
    "        f1 = getF1(\"4DL\",labels, preds)\n",
    "        if(f1>maxf1):\n",
    "            maxf1 = f1\n",
    "            max_y_test = labels\n",
    "            max_y_pred = preds\n",
    "\n",
    "    print(\"Max F1-Measure: \"+str(maxf1))\n",
    "    print_stats_percentage_train_test(\"4DL\", max_y_test, max_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f8fc8-e95d-4acb-ba1d-100c180acb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
